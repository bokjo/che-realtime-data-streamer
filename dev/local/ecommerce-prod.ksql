-- Create source connector for the products table in MySQL ecommerce DB

-- DROP CONNECTOR SOURCE_MYSQL_EC_PRODUCTS_PROD_01;
CREATE SOURCE CONNECTOR SOURCE_MYSQL_EC_PRODUCTS_PROD_01 WITH (
    'connector.class' = 'io.debezium.connector.mysql.MySqlConnector',
    'database.hostname' = '34.107.37.79',
    'database.port' = '3306',
    'database.user' = 'debezium',
    'database.password' = 'dbz',
    'database.server.id' = '52',
    'database.server.name' = 'che-ecommerce-prod',
    'table.whitelist' = 'ecommerce.products',
    'database.history.kafka.bootstrap.servers' = 'kafka:29092',
    'database.history.kafka.topic' = 'dbhistory.ecommerce-prod' ,
    'include.schema.changes' = 'false',
    'transforms'= 'unwrap,extractkey',
    'transforms.unwrap.type'= 'io.debezium.transforms.ExtractNewRecordState',
    'transforms.extractkey.type'= 'org.apache.kafka.connect.transforms.ExtractField$Key',
    'transforms.extractkey.field'= 'id',
    'key.converter'= 'org.apache.kafka.connect.storage.StringConverter',
    'value.converter'= 'io.confluent.connect.avro.AvroConverter',
    'value.converter.schema.registry.url'= 'http://schema-registry:8081'
    );

SHOW CONNECTORS;

SHOW TOPICS;

PRINT 'che-ecommerce-prod.ecommerce.products' FROM BEGINNING;

-- DROP CONNECTOR SOURCE_MYSQL_EC_ATTRIBUTES_PROD_01;
CREATE SOURCE CONNECTOR SOURCE_MYSQL_EC_ATTRIBUTES_PROD_01 WITH (
    'connector.class' = 'io.debezium.connector.mysql.MySqlConnector',
    'database.hostname' = '34.107.37.79',
    'database.port' = '3306',
    'database.user' = 'debezium',
    'database.password' = 'dbz',
    'database.server.id' = '53',
    'database.server.name' = 'che-ecommerce-prod',
    'table.whitelist' = 'ecommerce.attributes',
    'database.history.kafka.bootstrap.servers' = 'kafka:29092',
    'database.history.kafka.topic' = 'dbhistory.ecommerce-attributes-prod' ,
    'include.schema.changes' = 'false',
    'transforms'= 'unwrap,extractkey',
    'transforms.unwrap.type'= 'io.debezium.transforms.ExtractNewRecordState',
    'transforms.extractkey.type'= 'org.apache.kafka.connect.transforms.ExtractField$Key',
    'transforms.extractkey.field'= 'id',
    'key.converter'= 'org.apache.kafka.connect.storage.StringConverter',
    'value.converter'= 'io.confluent.connect.avro.AvroConverter',
    'value.converter.schema.registry.url'= 'http://schema-registry:8081'
    );

PRINT 'che-ecommerce-prod.ecommerce.attributes' FROM BEGINNING;

-- DROP CONNECTOR SOURCE_MYSQL_EC_ATTRIBUTES_KV_PROD_01;
CREATE SOURCE CONNECTOR SOURCE_MYSQL_EC_ATTRIBUTES_KV_PROD_01 WITH (
    'connector.class' = 'io.debezium.connector.mysql.MySqlConnector',
    'database.hostname' = '34.107.37.79',
    'database.port' = '3306',
    'database.user' = 'debezium',
    'database.password' = 'dbz',
    'database.server.id' = '54',
    'database.server.name' = 'che-ecommerce-prod',
    'table.whitelist' = 'ecommerce.attributes_kv',
    'database.history.kafka.bootstrap.servers' = 'kafka:29092',
    'database.history.kafka.topic' = 'dbhistory.ecommerce-attributes-kv-prod' ,
    'include.schema.changes' = 'false',
    'transforms'= 'unwrap,extractkey',
    'transforms.unwrap.type'= 'io.debezium.transforms.ExtractNewRecordState',
    'transforms.extractkey.type'= 'org.apache.kafka.connect.transforms.ExtractField$Key',
    'transforms.extractkey.field'= 'id',
    'key.converter'= 'org.apache.kafka.connect.storage.StringConverter',
    'value.converter'= 'io.confluent.connect.avro.AvroConverter',
    'value.converter.schema.registry.url'= 'http://schema-registry:8081'
    );

PRINT 'che-ecommerce-prod.ecommerce.attributes_kv' FROM BEGINNING;



CREATE TABLE PRODUCTS_PROD (PRODUCT_ID VARCHAR PRIMARY KEY)
  WITH (KAFKA_TOPIC='che-ecommerce-prod.ecommerce.products', VALUE_FORMAT='AVRO');

CREATE TABLE ATTRIBUTES_PROD (ATTRIBUTES_ID VARCHAR PRIMARY KEY)
  WITH (KAFKA_TOPIC='che-ecommerce-prod.ecommerce.attributes', VALUE_FORMAT='AVRO');

CREATE TABLE ATTRIBUTES_KV_PROD (ATTRIBUTES_KV_ID VARCHAR PRIMARY KEY)
  WITH (KAFKA_TOPIC='che-ecommerce-prod.ecommerce.attributes_kv', VALUE_FORMAT='AVRO');


SELECT TIMESTAMPTOSTRING(ROWTIME, 'HH:mm:ss') AS EVENT_TS,
       ATTRIBUTES_ID,
       DESCRIPTION
  FROM ATTRIBUTES_PROD WHERE ID=8
  EMIT CHANGES;

-- Create Products Stream...
CREATE STREAM PRODUCTS_STREAM_PROD (PRODUCT_ID VARCHAR KEY) WITH (KAFKA_TOPIC='che-ecommerce-prod.ecommerce.products', VALUE_FORMAT='AVRO');

SET 'auto.offset.reset' = 'earliest';

SELECT TIMESTAMPTOSTRING(ROWTIME, 'HH:mm:ss') AS EVENT_TS,
       PRODUCT_ID,
       ID,
       NAME,
       DESCRIPTION
  FROM PRODUCTS_STREAM_PROD WHERE ID=2
  EMIT CHANGES;  

  -- Create Attributes Stream...
CREATE STREAM ATTRIBUTES_STREAM_PROD (ATTRIBUTES_ID VARCHAR KEY) WITH (KAFKA_TOPIC='che-ecommerce-prod.ecommerce.attributes', VALUE_FORMAT='AVRO');

SELECT TIMESTAMPTOSTRING(ROWTIME, 'HH:mm:ss') AS EVENT_TS,
       ATTRIBUTES_ID,
       ID,
       DESCRIPTION
  FROM ATTRIBUTES_STREAM_PROD WHERE ID=2
  EMIT CHANGES;

-- Create AttributesKV Stream...
CREATE STREAM ATTRIBUTES_KV_STREAM_PROD (ATTRIBUTES_KV_ID VARCHAR KEY) WITH (KAFKA_TOPIC='che-ecommerce-prod.ecommerce.attributes_kv', VALUE_FORMAT='AVRO');

SELECT TIMESTAMPTOSTRING(ROWTIME, 'HH:mm:ss') AS EVENT_TS,
       ATTRIBUTES_KV_ID,
       ID,
       ATTRIBUTE
  FROM ATTRIBUTES_KV_STREAM_PROD WHERE ID=2
  EMIT CHANGES;


-- Join the products with atributes streams

-- SELECT * 
--   FROM ATTRIBUTES_PROD A
--  RIGHT JOIN ATTRIBUTES_KV_PROD AK
--     ON A.ID = AK.ATTRIBUTE
--   EMIT CHANGES; 


  SELECT * 
    FROM PRODUCTS_PROD P
   INNER JOIN ATTRIBUTES_PROD A
      ON P.PRODUCT_ID = A.ATTRIBUTES_ID
    EMIT CHANGES;  

-- TEMP FOR TESTING... ATTRIBUTES_KV need to be fixed with proper key per attribute!
  SELECT P.PRODUCT_ID, P.NAME, P.DESCRIPTION AS PRODUCT_DESCRIPTION,
         A.ATTRIBUTES_ID, A.DESCRIPTION AS ATTRIBUTE_DESCRIPTION,
         P.CREATE_TS AS PRODUCT_CREATE_TS, P.UPDATE_TS AS PRODUCT_UPDATE_TS
    FROM PRODUCTS_PROD P
   INNER JOIN ATTRIBUTES_PROD A
      ON P.PRODUCT_ID = A.ATTRIBUTES_ID
    EMIT CHANGES;  

  SELECT * 
    FROM ATTRIBUTES_KV AK
   LEFT JOIN ATTRIBUTES A
      ON A.ID = AK.ATTRIBUTE
    EMIT CHANGES; 


SET 'auto.offset.reset' = 'earliest';

CREATE STREAM PRODUCTS_WITH_ATTR_DATA_PROD_2
  WITH (KAFKA_TOPIC='products-with-attr-data-production-2') AS
SELECT P.PRODUCT_ID, P.NAME, P.DESCRIPTION AS PRODUCT_DESCRIPTION,
       A.ATTRIBUTES_ID, A.DESCRIPTION AS ATTRIBUTE_DESCRIPTION,
       P.CREATE_TS AS PRODUCT_CREATE_TS, P.UPDATE_TS AS PRODUCT_UPDATE_TS
 FROM PRODUCTS_STREAM_PROD P
INNER JOIN ATTRIBUTES_PROD A
   ON P.PRODUCT_ID = A.ATTRIBUTES_ID
 EMIT CHANGES;

 -- Create sink connector with ES and send the data over in real time...

 CREATE SINK CONNECTOR SINK_ELASTIC_PRODUCTS_WITH_ATTR_PROD_2_01 WITH (
  'connector.class' = 'io.confluent.connect.elasticsearch.ElasticsearchSinkConnector',
  'connection.url' = 'http://elasticsearch:9200',
  'type.name' = '',
  'behavior.on.malformed.documents' = 'warn',
  'errors.tolerance' = 'all',
  'errors.log.enable' = 'true',
  'errors.log.include.messages' = 'true',
  'topics' = 'products-with-attr-data-production-2',
  'key.ignore' = 'true',
  'schema.ignore' = 'true',
  'key.converter' = 'org.apache.kafka.connect.storage.StringConverter',
  'transforms'= 'ExtractTimestamp',
  'transforms.ExtractTimestamp.type'= 'org.apache.kafka.connect.transforms.InsertField$Value',
  'transforms.ExtractTimestamp.timestamp.field' = 'EXTRACT_TS'
);

 CREATE SINK CONNECTOR SINK_ELASTIC_PRODUCTS_ONLY_PROD_01 WITH (
  'connector.class' = 'io.confluent.connect.elasticsearch.ElasticsearchSinkConnector',
  'connection.url' = 'http://elasticsearch:9200',
  'type.name' = '',
  'behavior.on.malformed.documents' = 'warn',
  'errors.tolerance' = 'all',
  'errors.log.enable' = 'true',
  'errors.log.include.messages' = 'true',
  'topics' = 'che-ecommerce-prod.ecommerce.products',
  'key.ignore' = 'true',
  'schema.ignore' = 'true',
  'key.converter' = 'org.apache.kafka.connect.storage.StringConverter',
  'transforms'= 'ExtractTimestamp',
  'transforms.ExtractTimestamp.type'= 'org.apache.kafka.connect.transforms.InsertField$Value',
  'transforms.ExtractTimestamp.timestamp.field' = 'EXTRACT_TS'
);

 CREATE SINK CONNECTOR SINK_ELASTIC_ATTRIBUTES_ONLY_PROD_01 WITH (
  'connector.class' = 'io.confluent.connect.elasticsearch.ElasticsearchSinkConnector',
  'connection.url' = 'http://elasticsearch:9200',
  'type.name' = '',
  'behavior.on.malformed.documents' = 'warn',
  'errors.tolerance' = 'all',
  'errors.log.enable' = 'true',
  'errors.log.include.messages' = 'true',
  'topics' = 'che-ecommerce-prod.ecommerce.attributes',
  'key.ignore' = 'true',
  'schema.ignore' = 'true',
  'key.converter' = 'org.apache.kafka.connect.storage.StringConverter',
  'transforms'= 'ExtractTimestamp',
  'transforms.ExtractTimestamp.type'= 'org.apache.kafka.connect.transforms.InsertField$Value',
  'transforms.ExtractTimestamp.timestamp.field' = 'EXTRACT_TS'
);